MLOps (Cloud & Production)
==========================
*Supervised by Nicolas RAGOT and led by Théo BOISSEAU at the Ecole Polytechnique of the University of Tours.*


Goals
---------
The main objective of this project is to migrate a usual Machine Learning project into the Cloud (Azure Machine Learning) and then deploy it in a production environment through the key concepts of MLOps.

Organization
---------
1. A **Data Science** phase during which a classic Data Science project from a Jupyter Notebook must be validated and simplified to train a neural network on the MNIST image dataset.
2. A **Cloud** phase during which the code from this Notebook must then be migrated in the Cloud on Azure Machine Learning and configure the platform.
3. A **Production** phase during which the trained model must be deployed in a production environment according to MLOps best practices: automation, collaboration, monitoring and scalability.


Prerequisites
------------
The user must have Conda installed to manage their packages and a personal or school Azure Machine Learning subscription.
It can use a machine on Windows or Linux.


Facility
------------
To be executed, the entire *data_science* folder must be imported into the *Notebooks* tab of the *Authoring* part of Azure Machine Learning.

Regarding the main part of the project, **Production**, you must install the `production\model\conda.yaml` environment with the command:

    conda env create --name mlops --file production\model\conda.yaml
    source activate mlops


Project structure
-------------------

    mlops-demo
    ├── data_science/
    │   ├── input/
    │   │   ├── test.csv                # test data
    │   │   └── train.csv               # training data
    │   ├── working/
    │   │   ├── data_science_digits_model.ipynb    # Jupyter notebook of the main seript data science model
    │   │   └── main.py                 # main script
    │   └── cloud_AML_digits_model.ipynb  # Jupyter notebook for cloud deployment code for the production phase
    │
    ├── production/
    │   ├── foreign_data/
    │   │   ├── test.csv                # test data
    │   │   └── train.csv               # training data
    │   ├── model/
    │   │   ├── data/
    │   │   │   ├── model/
    │   │   │   │   ├── variables/
    │   │   │   │   ├── keras_metadata.pb  # Keras metadata file
    │   │   │   │   └── saved_model.pb  # saved model file
    │   │   │   ├── keras_module.txt    # Keras module text file
    │   │   │   └── save_format.txt     # backup format text file
    │   │   ├── MLmodel                 # MLFLow model
    │   │   ├── _summary.txt            # model summary
    │   │   ├── conda.yaml              # Conda environnement
    │   │   ├── python_env.yaml         # Pythonenvironnement
    │   │   └── requirements.txt        # Python dependencies
    │   │
    │   ├── working/
    │   │   ├── data_simulation.py      # data simulation script
    │   │   ├── dummy_server.py         # dummy server script
    │   │   ├── score.py                # scoring script
    │   │   └── visualization.py        # visualization script
    │   └── production.ipynb            # Jupyter notebook for the production phase
    │
    ├── .gitignore
    └── README.md

- `data_science/` contains the code for the **Data Science** and **Cloud** phases.
    The *input* folder contains the data from the MNIST dataset.
    The *working* folder contains the Notebook of a classic Data Science project adapted to the needs of the project.
    The *cloud_AML_digits_model.ipynb* Notebook contains the python instructions for Azure Machine Learning and the *main.py* file in the *working* folder corresponds to the code it generated and deployed to the platform.
    For the cloud, the entire *data_science* folder must be sent to Azure Machine Learning.

- `production/` contains the code for the **Production** phase.
    The *foreign_data* folder contains two datasets foreign to the Western MNIST dataset: one of numbers written by Indians and Nepalese, and another written by Japanese.
    The *model* folder is the output of the Azure Machine Learning training job: an artifact containing the environment needed to run the model, the serialized model, and metadata.
    The *production.ipynb* file is a Notebook in which a production environment is simulated and which presents an example of deployment of the model.
    Inside *working* is the *score.py* file (a scoring script) used for inference and generated by the *production.ipynb* Notebook, as well as the rest of the code used to simulate production.
